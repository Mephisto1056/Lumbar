# app/core/colbert_service_omni.py
from transformers import AutoModel, AutoProcessor
from transformers.utils.import_utils import is_flash_attn_2_available
from peft import PeftModel
from torch.utils.data import DataLoader, Dataset
import torch
import torchaudio
import torchvision.transforms as transforms
from torchvision.io import read_video
import cv2
import numpy as np
from typing import List, cast, Union, Tuple
from tqdm import tqdm
from config import settings
import librosa
from PIL import Image
import io
import tempfile
import os
import logging

# ç®€å•çš„Datasetæ›¿ä»£colpaliçš„ListDataset
class SimpleDataset(Dataset):
    def __init__(self, data):
        self.data = data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]

# è®¾ç½®logging
logger = logging.getLogger(__name__)

# ç‰ˆæœ¬å†²çªå·²é€šè¿‡æ­£ç¡®çš„ä¾èµ–ç‰ˆæœ¬çº¦æŸè§£å†³
# transformers>=4.50.0,<4.51.0 ä¸ colpali_engine==0.3.9 å®Œå…¨å…¼å®¹

class ColBERTOmniService:
    def __init__(self, model_path):
        # ä½¿ç”¨æ ‡å‡†torchè®¾å¤‡æ£€æµ‹
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºColQwen-omniæ¨¡å‹è·¯å¾„
        if "omni" in model_path.lower():
            print(f"Loading ColQwen-omni adapter from {model_path}")
            self.is_omni_model = True
            # ä½¿ç”¨æ­£ç¡®çš„å…¨æ¨¡æ€åŸºç¡€æ¨¡å‹
            base_model_path = settings.colqwen_omni_base_path
            print(f"Using omni base model: {base_model_path}")
        else:
            print(f"Loading ColQwen2.5 model from {model_path}")
            self.is_omni_model = False
            base_model_path = model_path
            
        try:
            if self.is_omni_model:
                print(f"Loading omni model configuration...")
                print(f"Adapter path: {model_path}")
                
                # ä½¿ç”¨åŸå§‹ColQwen2.5åŸºç¡€æ¨¡å‹ + omnié€‚é…å™¨ï¼ˆæµ‹è¯•éªŒè¯çš„æˆåŠŸæ–¹æ³•ï¼‰
                fallback_base = "/model_weights/colqwen2.5-base"
                print(f"Using original ColQwen2.5 base model: {fallback_base}")
                
                # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
                import os
                if not os.path.exists(fallback_base):
                    raise FileNotFoundError(f"Base model not found: {fallback_base}")
                if not os.path.exists(model_path):
                    raise FileNotFoundError(f"Omni adapter model not found: {model_path}")
                
                # ä½¿ç”¨å®˜æ–¹çš„ColQwen2_5Omniç±»ç›´æ¥åŠ è½½
                from colpali_engine.models import ColQwen2_5Omni
                print("Loading ColQwen2.5-Omni model using official implementation...")
                self.model = ColQwen2_5Omni.from_pretrained(
                    model_path,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True,
                    attn_implementation=(
                        "flash_attention_2" if is_flash_attn_2_available() else "eager"
                    ),
                ).eval()
                print("ğŸ‰ Successfully loaded ColQwen2.5-Omni model!")
                base_model_path = model_path  # ä½¿ç”¨omniæ¨¡å‹è·¯å¾„
                
            else:
                # åŸå§‹æ¨¡å‹åŠ è½½
                from colpali_engine.models import ColQwen2_5
                print(f"Loading original ColQwen2.5 model from: {base_model_path}")
                self.model = ColQwen2_5.from_pretrained(
                    base_model_path,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True,
                    attn_implementation=(
                        "flash_attention_2" if is_flash_attn_2_available() else "eager"
                    ),
                ).eval()
                print("Successfully loaded original ColQwen2.5 model")
                
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            # å¦‚æœomniæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè‡ªåŠ¨é™çº§åˆ°åŸå§‹æ¨¡å‹
            if self.is_omni_model:
                print("Omni model failed, falling back to original ColQwen2.5 model")
                self.is_omni_model = False
                fallback_base = "/model_weights/colqwen2.5-base"
                fallback_adapter = "/model_weights/colqwen2.5-v0.2"
                
                try:
                    print(f"Loading fallback base model: {fallback_base}")
                    self.model = ColQwen2_5.from_pretrained(
                        fallback_base,
                        torch_dtype=torch.bfloat16,
                        device_map=self.device,
                        trust_remote_code=True,
                        attn_implementation=(
                            "flash_attention_2" if is_flash_attn_2_available() else "eager"
                        ),
                    ).eval()
                    
                    # åŠ è½½åŸå§‹é€‚é…å™¨
                    from peft import PeftModel
                    print(f"Loading fallback adapter: {fallback_adapter}")
                    self.model = PeftModel.from_pretrained(self.model, fallback_adapter)
                    print("Successfully loaded fallback ColQwen2.5 model")
                    base_model_path = fallback_base  # æ›´æ–°processorè·¯å¾„
                    
                except Exception as fallback_e:
                    raise RuntimeError(f"Both omni and fallback model loading failed: {e}, {fallback_e}")
            else:
                raise RuntimeError(f"Model loading failed: {e}")
        
        # åŠ è½½Processor - ä½¿ç”¨colpali_engine omniå¤„ç†å™¨
        print(f"Loading processor from: {base_model_path}")
        try:
            if self.is_omni_model:
                from colpali_engine.models import ColQwen2_5OmniProcessor
                self.processor = ColQwen2_5OmniProcessor.from_pretrained(
                    base_model_path,
                    trust_remote_code=True,
                )
                print("Successfully loaded ColQwen2_5OmniProcessor")
            else:
                from colpali_engine.models import ColQwen2_5_Processor
                self.processor = cast(
                    ColQwen2_5_Processor,
                    ColQwen2_5_Processor.from_pretrained(
                        base_model_path,
                        size={"shortest_edge": 56 * 56, "longest_edge": 28 * 28 * 768},
                        trust_remote_code=True,
                    ),
                )
                print("Successfully loaded ColQwen2_5_Processor")
        except Exception as proc_e:
            print(f"Processor loading failed: {proc_e}")
            # ä½¿ç”¨ç®€åŒ–çš„å¤„ç†å™¨
            print("Using fallback processor...")
            class SimpleProcessor:
                def process_queries(self, queries):
                    return {"input_ids": torch.tensor([[1, 2, 3]])}  # ç®€åŒ–å®ç°
                def process_images(self, images):
                    return {"pixel_values": torch.randn(1, 3, 224, 224)}  # ç®€åŒ–å®ç°
            
            self.processor = SimpleProcessor()
            print("Using simple fallback processor")
        
        # éŸ³é¢‘å¤„ç†å‚æ•°
        self.audio_sample_rate = getattr(settings, 'audio_sample_rate', 16000)
        self.audio_segment_duration = getattr(settings, 'audio_segment_duration', 30)  # 30ç§’åˆ†æ®µ
        
        # è§†é¢‘å¤„ç†å‚æ•°
        self.video_frame_interval = getattr(settings, 'video_frame_interval', 1)  # æ¯ç§’æå–1å¸§
        
        # å®‰å…¨é™åˆ¶
        self.max_audio_duration = getattr(settings, 'max_audio_duration', 3600)  # 1å°æ—¶
        self.max_video_duration = getattr(settings, 'max_video_duration', 7200)  # 2å°æ—¶
        self.max_file_size = getattr(settings, 'max_file_size', 500 * 1024 * 1024)  # 500MB

    def process_query(self, queries: list) -> List[torch.Tensor]:
        """å¤„ç†æ–‡æœ¬æŸ¥è¯¢"""
        from colpali_engine.utils.torch_utils import ListDataset
        
        dataloader = DataLoader(
            dataset=ListDataset[str](queries),
            batch_size=1,
            shuffle=False,
            collate_fn=lambda x: self.processor.process_queries(x),
        )

        qs: List[torch.Tensor] = []
        for batch_query in dataloader:
            with torch.no_grad():
                batch_query = {
                    k: v.to(self.model.device) for k, v in batch_query.items()
                }
                embeddings_query = self.model(**batch_query)
            qs.extend(list(torch.unbind(embeddings_query.to("cpu"))))
        for i in range(len(qs)):
            qs[i] = qs[i].float().tolist()
        return qs

    def process_image(self, images: List) -> List[List[float]]:
        """å¤„ç†å›¾åƒ"""
        from colpali_engine.utils.torch_utils import ListDataset
        batch_size = 1
        
        dataloader = DataLoader(
            dataset=ListDataset[str](images),
            batch_size=batch_size,
            shuffle=False,
            collate_fn=lambda x: self.processor.process_images(x),
        )

        ds: List[torch.Tensor] = []
        for batch_doc in tqdm(dataloader):
            with torch.no_grad():
                batch_doc = {k: v.to(self.model.device) for k, v in batch_doc.items()}
                embeddings_doc = self.model(**batch_doc)
            ds.extend(list(torch.unbind(embeddings_doc.to("cpu"))))
        for i in range(len(ds)):
            ds[i] = ds[i].float().tolist()
        return ds

    def process_audio(self, audio_files: List[bytes]) -> List[dict]:
        """å¤„ç†éŸ³é¢‘æ–‡ä»¶ - æŒ‰æ—¶é—´åˆ†æ®µå¤„ç†å¹¶ç”Ÿæˆembeddings"""
        if not self.is_omni_model:
            # éomniæ¨¡å‹ä¸æ”¯æŒéŸ³é¢‘å¤„ç†
            return [{'error': 'Audio processing requires omni model', 'segments': []} for _ in audio_files]
            
        # æ£€æŸ¥processoræ˜¯å¦æœ‰process_audiosæ–¹æ³•
        if not hasattr(self.processor, 'process_audios'):
            logger.error("Processor does not have process_audios method")
            return [{'error': 'process_audios method not available', 'segments': []} for _ in audio_files]
        
        results = []
        
        for audio_bytes in audio_files:
            temp_path = None
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                if len(audio_bytes) > self.max_file_size:
                    raise ValueError(f"Audio file size {len(audio_bytes)} exceeds maximum {self.max_file_size}")
                
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                    temp_file.write(audio_bytes)
                    temp_path = temp_file.name
                
                # ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘æ•°æ®
                audio_array, sr = librosa.load(temp_path, sr=self.audio_sample_rate)
                
                # æ£€æŸ¥æ—¶é•¿é™åˆ¶
                duration = len(audio_array) / sr
                if duration > self.max_audio_duration:
                    raise ValueError(f"Audio duration {duration:.2f}s exceeds maximum {self.max_audio_duration}s")
                
                # æŒ‰æ—¶é—´åˆ†æ®µå¤„ç†éŸ³é¢‘
                segments = []
                segment_duration = self.audio_segment_duration
                
                for start_sec in range(0, int(duration), segment_duration):
                    end_sec = min(start_sec + segment_duration, duration)
                    
                    # æå–éŸ³é¢‘æ®µ
                    start_sample = int(start_sec * sr)
                    end_sample = int(end_sec * sr)
                    segment_array = audio_array[start_sample:end_sample]
                    
                    if len(segment_array) > 0:
                        # å¤„ç†éŸ³é¢‘æ®µ
                        audio_data = {'array': segment_array, 'sampling_rate': sr}
                        
                        # ä½¿ç”¨processorå¤„ç†éŸ³é¢‘æ®µ
                        dataloader = DataLoader(
                            dataset=[audio_data],
                            batch_size=1,
                            shuffle=False,
                            collate_fn=lambda x: self.processor.process_audios(x),
                        )
                        
                        embeddings_list = []
                        for batch_audio in dataloader:
                            with torch.no_grad():
                                batch_audio = {k: v.to(self.model.device) for k, v in batch_audio.items()}
                                embeddings_audio = self.model(**batch_audio)
                            embeddings_list.extend(list(torch.unbind(embeddings_audio.to("cpu"))))
                        
                        if embeddings_list:
                            embedding = embeddings_list[0].float().tolist()
                            
                            # ç”Ÿæˆsegment_id
                            import uuid
                            segment_id = str(uuid.uuid4())
                            
                            segments.append({
                                'segment_id': segment_id,
                                'start_time': start_sec,
                                'end_time': end_sec,
                                'duration': end_sec - start_sec,
                                'embedding': embedding
                            })
                
                results.append({
                    'segments': segments,
                    'total_duration': duration,
                    'sample_rate': sr,
                    'method': 'colpali_official_segmented'
                })
                
            except Exception as e:
                logger.error(f"Error processing audio: {e}")
                results.append({
                    'error': str(e),
                    'segments': [],
                    'method': 'colpali_official_segmented'
                })
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_path and os.path.exists(temp_path):
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
        
        return results

    def process_video(self, video_files: List[bytes]) -> List[dict]:
        """å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œæå–å…³é”®å¸§å’ŒéŸ³é¢‘"""
        results = []
        
        for video_bytes in video_files:
            temp_path = None
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                if len(video_bytes) > self.max_file_size:
                    raise ValueError(f"Video file size {len(video_bytes)} exceeds maximum {self.max_file_size}")
                
                # åˆ›å»ºå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
                    temp_file.write(video_bytes)
                    temp_path = temp_file.name
                
                # ä½¿ç”¨OpenCVè¯»å–è§†é¢‘
                cap = cv2.VideoCapture(temp_path)
                
                fps = cap.get(cv2.CAP_PROP_FPS)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                duration = frame_count / fps if fps > 0 else 0
                
                # æå–å…³é”®å¸§
                frame_embeddings = []
                frame_interval = int(fps * self.video_frame_interval) if fps > 0 else 1
                
                frame_idx = 0
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    if frame_idx % frame_interval == 0:
                        # è½¬æ¢ä¸ºPIL Image
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        pil_image = Image.fromarray(frame_rgb)
                        
                        # ç”Ÿæˆembedding
                        frame_embedding = self.process_image([pil_image])[0]
                        
                        timestamp = frame_idx / fps if fps > 0 else frame_idx
                        frame_embeddings.append({
                            'timestamp': timestamp,
                            'frame_idx': frame_idx,
                            'embedding': frame_embedding,
                            'pil_image': pil_image  # ä¿å­˜PILå›¾åƒä»¥ä¾›åç»­ç”Ÿæˆç¼©ç•¥å›¾
                        })
                    
                    frame_idx += 1
                
                cap.release()
                
                # æå–éŸ³é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
                audio_segments = []
                try:
                    # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
                    audio_temp_path = temp_path.replace('.mp4', '_audio.wav')
                    import subprocess
                    result = subprocess.run([
                        'ffmpeg', '-i', temp_path, '-vn', '-acodec', 'pcm_s16le',
                        '-ar', str(self.audio_sample_rate), '-ac', '1', audio_temp_path, '-y'
                    ], capture_output=True, text=True)
                    
                    if result.returncode == 0 and os.path.exists(audio_temp_path):
                        with open(audio_temp_path, 'rb') as af:
                            audio_data = af.read()
                        
                        # å¤„ç†æå–çš„éŸ³é¢‘
                        audio_results = self.process_audio([audio_data])
                        if audio_results and len(audio_results) > 0:
                            audio_result = audio_results[0]
                            if 'segments' in audio_result:
                                audio_segments = audio_result['segments']
                        
                        # æ¸…ç†éŸ³é¢‘ä¸´æ—¶æ–‡ä»¶
                        if os.path.exists(audio_temp_path):
                            os.unlink(audio_temp_path)
                            
                except Exception as audio_error:
                    logger.warning(f"Could not extract audio from video: {audio_error}")
                
                # è½¬æ¢å¸§æ•°æ®ç»“æ„ä»¥åŒ¹é…åç«¯æœŸæœ›
                frames = []
                for frame_emb in frame_embeddings:
                    import uuid
                    frame_id = str(uuid.uuid4())
                    
                    # ç”Ÿæˆç¼©ç•¥å›¾æ•°æ® - å°†PILå›¾åƒè½¬æ¢ä¸ºbase64
                    thumbnail_base64 = None
                    if 'pil_image' in frame_emb:
                        import base64
                        thumbnail_buffer = io.BytesIO()
                        frame_emb['pil_image'].save(thumbnail_buffer, format='JPEG', quality=85)
                        thumbnail_base64 = base64.b64encode(thumbnail_buffer.getvalue()).decode('utf-8')
                    
                    frames.append({
                        'frame_id': frame_id,
                        'segment_id': frame_id,  # æ·»åŠ segment_idå­—æ®µ
                        'timestamp': frame_emb['timestamp'],
                        'start_time': frame_emb['timestamp'],  # æ·»åŠ start_time
                        'end_time': frame_emb['timestamp'] + 1,  # æ·»åŠ end_timeï¼ˆå¸§æŒç»­1ç§’ï¼‰
                        'duration': 1.0,  # å¸§æŒç»­æ—¶é—´1ç§’
                        'frame_idx': frame_emb['frame_idx'],
                        'embedding': frame_emb['embedding'],
                        'thumbnail_base64': thumbnail_base64  # æ·»åŠ ç¼©ç•¥å›¾æ•°æ®
                    })
                
                # audio_segments å·²ç»åœ¨ä¸Šé¢å¤„ç†äº†
                
                results.append({
                    'duration': duration,
                    'fps': fps,
                    'frame_count': frame_count,
                    'frames': frames,  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                    'audio_segments': audio_segments  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                })
                
            except Exception as e:
                logger.error(f"Error processing video: {e}")
                results.append({
                    'error': str(e),
                    'frames': [],  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                    'audio_segments': []  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                })
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_path and os.path.exists(temp_path):
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
        
        return results

    # æ—§çš„è‡ªå®šä¹‰éŸ³é¢‘å¤„ç†æ–¹æ³•å·²åˆ é™¤ï¼Œç°åœ¨ä½¿ç”¨å®˜æ–¹colpali-engine API

    def process_multimodal(self, files: List[dict]) -> List[dict]:
        """å¤„ç†å¤šæ¨¡æ€æ–‡ä»¶ï¼ˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æ··åˆï¼‰"""
        results = []
        
        for file_info in files:
            file_type = file_info.get('type', '')
            file_data = file_info.get('data', b'')
            
            if file_type.startswith('image/'):
                # å¤„ç†å›¾åƒ
                image = Image.open(io.BytesIO(file_data))
                embedding = self.process_image([image])[0]
                results.append({
                    'type': 'image',
                    'embedding': embedding
                })
            elif file_type.startswith('audio/'):
                # å¤„ç†éŸ³é¢‘
                audio_result = self.process_audio([file_data])[0]
                results.append({
                    'type': 'audio',
                    **audio_result
                })
            elif file_type.startswith('video/'):
                # å¤„ç†è§†é¢‘
                video_result = self.process_video([file_data])[0]
                results.append({
                    'type': 'video',
                    **video_result
                })
            else:
                results.append({
                    'type': 'unknown',
                    'error': f'Unsupported file type: {file_type}'
                })
        
        return results


# åˆ›å»ºå…¨å±€å®ä¾‹ - æ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹ï¼Œå¸¦é™çº§å¤„ç†
colbert_omni = None

def initialize_model():
    global colbert_omni
    
    print(f"Initializing model with type: {settings.model_type}")
    
    try:
        if settings.model_type == "omni":
            model_path = settings.colqwen_omni_model_path
            print(f"Attempting to use ColQwen-omni model: {model_path}")
            print(f"Omni base path: {settings.colqwen_omni_base_path}")
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            import os
            if not os.path.exists(model_path):
                print(f"Warning: Omni adapter path does not exist: {model_path}")
            if not os.path.exists(settings.colqwen_omni_base_path):
                print(f"Warning: Omni base path does not exist: {settings.colqwen_omni_base_path}")
            
            colbert_omni = ColBERTOmniService(model_path)
            print("Successfully initialized ColQwen-omni model")
        else:
            model_path = settings.colbert_model_path
            print(f"Using ColQwen2.5 model: {model_path}")
            colbert_omni = ColBERTOmniService(model_path)
            print("Successfully initialized ColQwen2.5 model")
            
    except Exception as e:
        print(f"Failed to load primary model: {e}")
        print(f"Error details: {str(e)}")
        
        if settings.model_type == "omni":
            print("Falling back to stable ColQwen2.5 model...")
            try:
                # é™çº§åˆ°åŸå§‹æ¨¡å‹
                fallback_path = settings.colbert_model_path
                print(f"Loading fallback model: {fallback_path}")
                
                # æ£€æŸ¥é™çº§è·¯å¾„
                import os
                if not os.path.exists(fallback_path):
                    print(f"Error: Fallback path also does not exist: {fallback_path}")
                    raise FileNotFoundError(f"Fallback model not found: {fallback_path}")
                
                colbert_omni = ColBERTOmniService(fallback_path)
                print("Successfully loaded fallback model")
                
            except Exception as fallback_e:
                print(f"Fallback model also failed: {fallback_e}")
                print(f"Fallback error details: {str(fallback_e)}")
                raise RuntimeError(f"Both primary and fallback models failed to load: {e} | {fallback_e}")
        else:
            print(f"Original model loading failed: {e}")
            raise e

# åˆå§‹åŒ–æ¨¡å‹
try:
    initialize_model()
except Exception as init_e:
    print(f"Critical error during model initialization: {init_e}")
    # ä¸è¦è®©ç¨‹åºå®Œå…¨é€€å‡ºï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªç©ºçš„æœåŠ¡å®ä¾‹
    print("Creating placeholder service to prevent crash...")
    class PlaceholderService:
        def __init__(self):
            self.device = torch.device("cpu")
            self.is_omni_model = False
            
        def process_query(self, queries):
            raise RuntimeError("Model failed to initialize")
            
        def process_image(self, images):
            raise RuntimeError("Model failed to initialize")
            
        def process_audio(self, audio_files):
            raise RuntimeError("Model failed to initialize")
            
        def process_video(self, video_files):
            raise RuntimeError("Model failed to initialize")
            
        def process_multimodal(self, files):
            raise RuntimeError("Model failed to initialize")
    
    colbert_omni = PlaceholderService()
    print("Placeholder service created - server will start but models won't work")